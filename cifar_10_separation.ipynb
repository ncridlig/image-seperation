{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE7oyG0wv6e0"
   },
   "source": [
    "# Separation of CIFAR-10 Images\n",
    "\n",
    "The model takes as input an image created by averaging two random samples from CIFAR-10 and is tasked with predicting the categories of the two components.\n",
    "\n",
    "**For sure it is a computer vision model.**\n",
    "\n",
    "The first image belongs to the first five categories (airplane, automobile, bird, cat, deer), while the second belongs to the remaining categories (dog, frog, horse, ship, truck). The model must return two labels, each within a range of five possible values.\n",
    "\n",
    "**(5->1, 5->1)**\n",
    "\n",
    "The evaluation metric for the model is as follows: calculate the classification accuracy for the two component images and then compute their average.\n",
    "\n",
    "**accuracy = acc1 + acc2 / 2**\n",
    "**where acc1 = (correct1/total) and acc2 = (correct2/total)**\n",
    "\n",
    "The metric should be evaluated on 10,000 inputs generated from test data. Repeat the calculation 10 times and measure the standard deviation, which must be reported.\n",
    "\n",
    "A data generator and some examples are provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USdmzjiO0W6D"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iHjnh5XP0Sq4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.nn import fractional_max_pool\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from keras.saving import load_model, save_model, save_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yRYiW2ipukZF",
    "outputId": "0d5c8981-51da-4c92-de73-84bcac11aa2c"
   },
   "outputs": [],
   "source": [
    "(cifar10_x_train, cifar10_y_train), (cifar10_x_test, cifar10_y_test) = cifar10.load_data()\n",
    "assert cifar10_x_train.shape == (50000, 32, 32, 3)\n",
    "assert cifar10_x_test.shape == (10000, 32, 32, 3)\n",
    "assert cifar10_y_train.shape == (50000, 1)\n",
    "assert cifar10_y_test.shape == (10000, 1)\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "cifar10_x_train, cifar10_x_val, cifar10_y_train, cifar10_y_val = train_test_split(\n",
    "    cifar10_x_train, cifar10_y_train, test_size=0.1\n",
    ")\n",
    "\n",
    "# First classifier: \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\"\n",
    "# Second classifier: \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "# IDEA: extract the features with a CNN backbone and feed them to two seperate FCN classifiers. How about backpropogation?\n",
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# Normalizing to range (0,1)\n",
    "cifar10_x_train = (cifar10_x_train/255.).astype(np.float32)\n",
    "cifar10_x_val = (cifar10_x_val / 255.).astype(np.float32)\n",
    "cifar10_x_test = (cifar10_x_test/255.).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkiGnU4d0k4d"
   },
   "source": [
    "Let us split the images in two groups, according to their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Dpey42Vo07Yb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22513, 32, 32, 3) (22513, 1)\n",
      "(22487, 32, 32, 3) (22487, 1)\n",
      "(2487, 32, 32, 3) (2487, 1)\n",
      "(2513, 32, 32, 3) (2513, 1)\n",
      "(5000, 32, 32, 3) (5000, 1)\n",
      "(5000, 32, 32, 3) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "cond_1 = cifar10_y_train[:,0] < 5\n",
    "cifar10_x_train_1 = cifar10_x_train[cond_1]\n",
    "cifar10_y_train_1 = cifar10_y_train[cond_1]\n",
    "\n",
    "cond_2 = cifar10_y_train[:,0] >= 5\n",
    "cifar10_x_train_2 = cifar10_x_train[cond_2]\n",
    "cifar10_y_train_2 = cifar10_y_train[cond_2]\n",
    "\n",
    "cond_1_val = cifar10_y_val[:,0] < 5\n",
    "cifar10_x_val_1 = cifar10_x_val[cond_1_val]\n",
    "cifar10_y_val_1 = cifar10_y_val[cond_1_val]\n",
    "\n",
    "cond_2_val = cifar10_y_val[:,0] >= 5\n",
    "cifar10_x_val_2 = cifar10_x_val[cond_2_val]\n",
    "cifar10_y_val_2 = cifar10_y_val[cond_2_val]\n",
    "\n",
    "cond_1_test = cifar10_y_test[:,0] < 5\n",
    "cifar10_x_test_1 = cifar10_x_test[cond_1_test]\n",
    "cifar10_y_test_1 = cifar10_y_test[cond_1_test]\n",
    "\n",
    "cond_2_test = cifar10_y_test[:,0] >= 5\n",
    "cifar10_x_test_2 = cifar10_x_test[cond_2_test]\n",
    "cifar10_y_test_2 = cifar10_y_test[cond_2_test]\n",
    "\n",
    "print(cifar10_x_train_1.shape, cifar10_y_train_1.shape)\n",
    "print(cifar10_x_train_2.shape, cifar10_y_train_2.shape)\n",
    "print(cifar10_x_val_1.shape, cifar10_y_val_1.shape)\n",
    "print(cifar10_x_val_2.shape, cifar10_y_val_2.shape)\n",
    "print(cifar10_x_test_1.shape, cifar10_y_test_1.shape)\n",
    "print(cifar10_x_test_2.shape, cifar10_y_test_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmLYNuR-0s0m"
   },
   "source": [
    "Now we can define the generator. The input consists of two datasets (X1,X2), their corresponding labels (Y1,Y2), and a batch size.\n",
    "\n",
    "**Model input: (B x X1\\*X2)**\n",
    "**Model output: (B x Y1 x Y2)**\n",
    "\n",
    "The generator returns (x_data,y_data), where:\n",
    "* x_data is a batch of images obtained by averaging random samples from X1 and X2.\n",
    "* y_data is a pair of batches of labels corresponding to the component images, expressed in categorical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "\n",
    "def datagenerator(X1,X2,Y1,Y2,batchsize):\n",
    "  size1 = X1.shape[0]\n",
    "  size2 = X2.shape[0]\n",
    "  # Convert the integer labels into one hot encoded vectors for cross entropy loss\n",
    "  # Careful: since there are two different sets of labels, I need to code two seperate outputs with seperate cross entropy losses\n",
    "  # Keras (TF) can handle this with: model = models.Model(inputs=merged_input_image, outputs=[output1, output2])\n",
    "  Y1_cat = tf.keras.utils.to_categorical(Y1, num_classes=5)\n",
    "  Y2_cat = tf.keras.utils.to_categorical(Y2-5, num_classes=5)\n",
    "\n",
    "  while True:\n",
    "    # Random image selections\n",
    "    num1 = np.random.randint(0, size1, batchsize)\n",
    "    num2 = np.random.randint(0, size2, batchsize)\n",
    "    # Average image production\n",
    "    x_data = (X1[num1] + X2[num2]) / 2.0\n",
    "    # Dictionary for y_data with keys matching the model\n",
    "    y_data = {'output1': Y1_cat[num1], 'output2': Y2_cat[num2]}\n",
    "    # Convert numpy to tf tensor\n",
    "    yield tf.convert_to_tensor(x_data), y_data\n",
    "\n",
    "traingen = datagenerator(cifar10_x_train_1,cifar10_x_train_2,cifar10_y_train_1,cifar10_y_train_2,batchsize)\n",
    "valgen = datagenerator(cifar10_x_val_1,cifar10_x_val_2,cifar10_y_val_1,cifar10_y_val_2,batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9lf3TuP2pdQ"
   },
   "source": [
    "\n",
    "Let us instantiate a generator on Cifar10 with batchsize=1, and let's see its behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "29TldJ6-720b"
   },
   "outputs": [],
   "source": [
    "datagen = datagenerator(cifar10_x_train_1,cifar10_x_train_2,cifar10_y_train_1,cifar10_y_train_2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1DrJVzI3ysV"
   },
   "source": [
    "Let's generate an example, display the image that the model will take as input, and print the categories of the two overlapping components.\n",
    "\n",
    "You can re-run the cell to display new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "qL1sMtjG8VmG",
    "outputId": "5b9d6599-d407-4e32-dad7-f3f95890f0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first: airplane, second = ship\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17e542280>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = next(datagen)\n",
    "\n",
    "print(\"first: {}, second = {}\".format(classes[np.argmax(y['output1'][0])],classes[np.argmax(y['output2'][0])+5]))\n",
    "#print(np.min(x[0]),np.max(x[0]))\n",
    "plt.imshow(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5lzBotwL5QN"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_p4UuG1QF8t"
   },
   "source": [
    "Let us define first of all the test generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQo8_6w-L4WY",
    "outputId": "a626a55c-7df8-456b-adcd-580d4166e184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "testgen = datagenerator(cifar10_x_test_1,cifar10_x_test_2,cifar10_y_test_1,cifar10_y_test_2,10000)\n",
    "\n",
    "eval_samples_x, eval_samples_y = next(testgen)\n",
    "print(eval_samples_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MiLnkKROGCD"
   },
   "source": [
    "We now test a model producing random guesses. You will need to replace it with your own predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1GllTEtPN_xv"
   },
   "outputs": [],
   "source": [
    "def random_model(x):\n",
    "  #the random model ignores the input x and return a pair of random classes\n",
    "  # 10,000 examples in a 2 column array between 0 and 4 inclusive\n",
    "  return(np.random.randint(0,5,(10000,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gomFTuuDOy8A"
   },
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "  eval_samples_x, eval_samples_y = next(testgen)\n",
    "  random_guesses = model(eval_samples_x)\n",
    "  correct_guesses_1 = random_guesses[:,0] == np.argmax(eval_samples_y['output1'],axis=1)\n",
    "  correct_guesses_2 = random_guesses[:,1] == np.argmax(eval_samples_y['output2'],axis=1)\n",
    "  return (np.mean(correct_guesses_1) + np.mean(correct_guesses_2))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4AL2M6yjJno",
    "outputId": "d84ecfb8-299f-4e37-85ee-c69b83a39e0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20655"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(random_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7usBI88dje70"
   },
   "source": [
    "As expected, the accuracy is around 1/5 = 0.2\n",
    "\n",
    "Let us repeat the evaluation ten times, and compute the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFu8iEt9jdZA",
    "outputId": "93fa99f4-972a-4795-917b-b1db97dc7ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy =  0.199285\n",
      "standard deviation =  0.0022481158777963414\n"
     ]
    }
   ],
   "source": [
    "repeat_eval = 10\n",
    "eval_results = []\n",
    "for i in range(repeat_eval):\n",
    "  eval_results.append(eval_model(random_model))\n",
    "print(\"mean accuracy = \", np.mean(eval_results))\n",
    "print(\"standard deviation = \", np.std(eval_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1yTRAzn4i9g"
   },
   "source": [
    "# What to Submit\n",
    "\n",
    "As usual, you need to submit a single notebook that must be executable on Colab. The notebook should be properly commented and include a complete record of the training process, as well as the calculation of accuracy according to the guidelines provided above.\n",
    "\n",
    "# Good luck!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I googled CIFAR-10 best accuracy and I fell upon a reddit post about a person trying to train it as fast as possible, [here](https://github.com/tysam-code/hlb-CIFAR10). Since I am not interested in light speed CIFAR-10, I googled only CIFAR-10 and arrived to the main website for this [dataset](https://www.cs.toronto.edu/~kriz/cifar.html). From there, I found a non-working link to Rodrigo Benenson since github.com has been deprecated and is now github.io, changing this and selecting \"who is the best in CIFAR-10 ?\" brought me to the [world rankings](https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130). Now I will read these papers in order. The first is [Fractional Max Pooling](https://arxiv.org/abs/1412.6071), with an accuracy of 96.53%. As I read this, I think, is my task the same? As a human, can I learn to distinguish the two classes? For sure, I will have to use the generator to see.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'fmp', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"fmp\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"fmp\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ ?                      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">820</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,430</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,840</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,050</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ ?                      │           \u001b[38;5;34m130\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │           \u001b[38;5;34m820\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │         \u001b[38;5;34m2,430\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │         \u001b[38;5;34m4,840\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │         \u001b[38;5;34m2,050\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │        \u001b[38;5;34m18,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,280</span> (110.47 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m28,280\u001b[0m (110.47 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,280</span> (110.47 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m28,280\u001b[0m (110.47 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define my fractional pooling model\n",
    "\n",
    "class FMP(Model):\n",
    "    def __init__(self):\n",
    "        super(FMP, self).__init__()\n",
    "        # Input\n",
    "        #self.input_layer = layers.InputLayer(shape=(32, 32, 3))  # For CIFAR-10 images\n",
    "\n",
    "        # Four convolutional layers (C2 layers). Each convolutional layer should have an increasing number of filters, starting from 10n filters.\n",
    "        self.conv1 = layers.Conv2D(10, (2, 2), activation=LeakyReLU())\n",
    "        self.conv2 = layers.Conv2D(20, (2, 2), activation=LeakyReLU())\n",
    "        self.conv3 = layers.Conv2D(30, (2, 2), activation=LeakyReLU())\n",
    "        self.conv4 = layers.Conv2D(40, (2, 2), activation=LeakyReLU())\n",
    "        self.conv5 = layers.Conv2D(50, (1, 1), activation=LeakyReLU())\n",
    "\n",
    "        # Three fractional max pooling layers\n",
    "        self.pooling_ratio1 = [1.0, 3/2, 3/2, 1.0]\n",
    "        self.pooling_ratio2 = [1.0, 6/4, 6/4, 1.0]\n",
    "        self.pooling_ratio3 = [1.0, 10/7, 10/7, 1.0]\n",
    "\n",
    "        # Linearly increasing dropout in the hidden layers\n",
    "        #self.dropout1 = layers.Dropout(0.0)  # 0% dropout in the first hidden layer\n",
    "        self.dropout = layers.Dropout(0.5)  # 50% dropout in the final hidden layer\n",
    "\n",
    "        # Classifier\n",
    "        self.fc1 = layers.Dense(10, activation='softmax')  # CIFAR-10 (10 classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Input\n",
    "        #x = self.input_layer(inputs)\n",
    "\n",
    "        # Feature Extractor\n",
    "        x = self.conv1(inputs)\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio1, pseudo_random=True, overlapping=True)\n",
    "        x = self.conv2(x)\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio2, pseudo_random=True, overlapping=True)\n",
    "        x = self.conv3(x)\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio3, pseudo_random=True, overlapping=True)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        # Classifier\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)  # CIFAR-10\n",
    "\n",
    "        return x\n",
    "\n",
    "K.clear_session()\n",
    "model = FMP()\n",
    "model.build(input_shape=(None, 32, 32, 3))  # Adjust if needed\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "model(tf.random.normal((1, 32, 32, 3)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(45000, 10)\n"
     ]
    }
   ],
   "source": [
    "Y_cat_train = tf.keras.utils.to_categorical(cifar10_y_train, num_classes=10)\n",
    "Y_cat_test = tf.keras.utils.to_categorical(cifar10_y_test, num_classes=10)\n",
    "print(cifar10_x_train.shape)\n",
    "print(Y_cat_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 549ms/step - accuracy: 0.1475 - loss: 2.2588\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=cifar10_x_train, y=Y_cat_train, epochs=1, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2547 - loss: 2.0465\n",
      "fmp Loss: 2.0488, Accuracy: 25.46%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25459998846054077"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate\n",
    "def evaluate(model, x, y):\n",
    "    loss, accuracy = model.evaluate(x, y)\n",
    "    print(f\"{model.name} Loss: {loss:.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "evaluate(model, x=cifar10_x_test, y=Y_cat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing this, I feel like the convolutional layers would benefit from being more complex and so I use this [person's](https://github.com/WingsBrokenAngel/fractional_max_pooling_and_recurrent_convolutional_neural_network) filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'fmp1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"fmp1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"fmp1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,260</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">21,690</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">43,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,150</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">54,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │           \u001b[38;5;34m390\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │         \u001b[38;5;34m7,260\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │        \u001b[38;5;34m21,690\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │        \u001b[38;5;34m43,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │        \u001b[38;5;34m18,150\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │        \u001b[38;5;34m54,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">144,820</span> (565.70 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m144,820\u001b[0m (565.70 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">144,820</span> (565.70 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m144,820\u001b[0m (565.70 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class FMP1(Model):\n",
    "    def __init__(self, FILTERS):\n",
    "        super(FMP1, self).__init__()\n",
    "        # Parameters\n",
    "        self.filters = FILTERS\n",
    "\n",
    "        # Four convolutional layers (C2 layers). Each convolutional layer should have an increasing number of filters, starting from 10n filters.\n",
    "        self.conv1 = layers.Conv2D(10*FILTERS, (2, 2), activation=LeakyReLU())\n",
    "        self.conv2 = layers.Conv2D(20*FILTERS, (2, 2), activation=LeakyReLU())\n",
    "        self.conv3 = layers.Conv2D(30*FILTERS, (2, 2), activation=LeakyReLU())\n",
    "        self.conv4 = layers.Conv2D(40*FILTERS, (2, 2), activation=LeakyReLU())\n",
    "        self.conv5 = layers.Conv2D(50*FILTERS, (1, 1), activation=LeakyReLU())\n",
    "\n",
    "        # Three fractional max pooling layers\n",
    "        self.pooling_ratio1 = [1.0, 3/2, 3/2, 1.0]\n",
    "        self.pooling_ratio2 = [1.0, 6/4, 6/4, 1.0]\n",
    "        self.pooling_ratio3 = [1.0, 10/7, 10/7, 1.0]\n",
    "\n",
    "        # Linearly increasing dropout in the hidden layers\n",
    "        #self.dropout1 = layers.Dropout(0.0)  # 0% dropout in the first hidden layer\n",
    "        self.dropout = layers.Dropout(0.5)  # 50% dropout in the final hidden layer\n",
    "\n",
    "        # Classifier\n",
    "        self.fc1 = layers.Dense(10, activation='softmax')  # CIFAR-10 (10 classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Input\n",
    "        #x = self.input_layer(inputs)\n",
    "\n",
    "        # Feature Extractor\n",
    "        x = self.conv1(inputs)\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio1, pseudo_random=True, overlapping=True)\n",
    "        x = self.conv2(x)\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio2, pseudo_random=True, overlapping=True)\n",
    "        x = self.conv3(x)\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio3, pseudo_random=True, overlapping=True)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        # Classifier\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)  # CIFAR-10\n",
    "\n",
    "        return x\n",
    "\n",
    "#K.clear_session()\n",
    "model = FMP1(FILTERS=3)\n",
    "model.build(input_shape=(None, 32, 32, 3))  # Adjust if needed\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "model(tf.random.normal((1, 32, 32, 3)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.1188 - loss: 2.5421\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=cifar10_x_train, y=Y_cat_train, epochs=1, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.2040 - loss: 2.1521\n",
      "fmp1 Loss: 2.1532, Accuracy: 20.36%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20360000431537628"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, x=cifar10_x_test, y=Y_cat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decide to implement a network as similar as possible, now that I understand the architecture better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMP2(Model):\n",
    "    def __init__(self):\n",
    "        super(FMP2, self).__init__()\n",
    "        # Input layer size 36x36\n",
    "\n",
    "        # Six convolutional layers (C2 layers). Each convolutional layer should have an increasing number of filters, starting from 10n filters.\n",
    "        self.conv1 = layers.Conv2D(32, (2, 2), activation=LeakyReLU()) # 36x36 # comment left here to show how the first time I calculated them wrong, both by thinking the input is 36 when it is 32 and forgetting conv takes 1 \n",
    "        self.conv2 = layers.Conv2D(64, (2, 2), activation=LeakyReLU()) # 25x25\n",
    "        self.conv3 = layers.Conv2D(96, (2, 2), activation=LeakyReLU()) # 18x18\n",
    "        self.conv4 = layers.Conv2D(128, (2, 2), activation=LeakyReLU()) # 13x13\n",
    "        self.conv5 = layers.Conv2D(160, (2, 2), activation=LeakyReLU()) # 9x9\n",
    "        self.conv6 = layers.Conv2D(192, (2, 2), activation=LeakyReLU()) # 6x6\n",
    "\n",
    "        # Six fractional max pooling layers\n",
    "        self.pooling_ratio1 = [1.0, 31/22, 31/22, 1.0]\n",
    "        self.pooling_ratio2 = [1.0, 21/15, 21/15, 1.0]\n",
    "        self.pooling_ratio3 = [1.0, 14/10, 14/10, 1.0]\n",
    "        self.pooling_ratio4 = [1.0, 9/6, 9/6, 1.0]\n",
    "        self.pooling_ratio5 = [1.0, 5/4, 5/4, 1.0]\n",
    "        self.pooling_ratio6 = [1.0, 3/2, 3/2, 1.0]\n",
    "\n",
    "        # Two final convolutional layers\n",
    "        self.conv7 = layers.Conv2D(192, (2, 2), activation=LeakyReLU()) # 4x4\n",
    "        self.conv8 = layers.Conv2D(192, (1, 1), activation=LeakyReLU())\n",
    "\n",
    "        # Linearly increasing dropout in the hidden layers\n",
    "        self.dropout = layers.Dropout(0.5)  # 50% dropout in the final hidden layer\n",
    "\n",
    "        # Classifier \n",
    "        self.fc = layers.Dense(10, activation='softmax')  # CIFAR-10 (10 classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Feature Extractor\n",
    "        x = self.conv1(inputs) # 31x31\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio1, pseudo_random=True, overlapping=True) # 22x22\n",
    "        x = self.conv2(x) # 21x21\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio2, pseudo_random=True, overlapping=True) # 15x15\n",
    "        x = self.conv3(x) # 14x14\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio3, pseudo_random=True, overlapping=True) # 10x10\n",
    "        x = self.conv4(x) # 9x9\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio4, pseudo_random=True, overlapping=True) # 6x6\n",
    "        x = self.conv5(x) #5x5\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio5, pseudo_random=True, overlapping=True) # 4x4\n",
    "        x = self.conv6(x) # 3x3\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio6, pseudo_random=True, overlapping=True) # 2x2\n",
    "        x = self.conv7(x) # 1x1\n",
    "        # IDEA: dropout here at 25%\n",
    "        x = self.conv8(x) #1x1\n",
    "\n",
    "        # Classifier\n",
    "        x = layers.Flatten()(x) # future improvement might be to use global average pooling to lower the FCN parameter count\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)  # CIFAR-10\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'fmp2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"fmp2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"fmp2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ ?                      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,672</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">82,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">123,072</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,648</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,930</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ ?                      │           \u001b[38;5;34m416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │        \u001b[38;5;34m24,672\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │        \u001b[38;5;34m82,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │       \u001b[38;5;34m123,072\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │       \u001b[38;5;34m147,648\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │        \u001b[38;5;34m37,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │         \u001b[38;5;34m1,930\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">474,410</span> (1.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m474,410\u001b[0m (1.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">474,410</span> (1.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m474,410\u001b[0m (1.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = FMP2()\n",
    "model.build(input_shape=(None, 32, 32, 3))  # Adjust if needed\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "model(tf.random.normal((1, 32, 32, 3)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - accuracy: 0.1003 - loss: 14.8851\n"
     ]
    }
   ],
   "source": [
    "Y_cat_train = tf.keras.utils.to_categorical(cifar10_y_train, num_classes=10)\n",
    "\n",
    "history = model.fit(x=cifar10_x_train, y=Y_cat_train, epochs=1, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.1286 - loss: 2.3659\n",
      "fmp2 Loss: 2.3636, Accuracy: 12.55%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12549999356269836"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, x=cifar10_x_test, y=Y_cat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without checkpoints or early stop, the model clearly overfit!\n",
    "\n",
    "Now I realize I forgot to read section 4.4 and implement that network! However, before, I try to move my network from the proxy task to the real one by adding a second classifier head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class FMP3(Model):\n",
    "    def __init__(self):\n",
    "        super(FMP3, self).__init__()\n",
    "        # Input layer size 32x32\n",
    "\n",
    "        # Six convolutional layers (C2 layers). Each convolutional layer should have an increasing number of filters, starting from 10n filters.\n",
    "        self.conv1 = layers.Conv2D(32, (2, 2), activation=LeakyReLU())\n",
    "        self.conv2 = layers.Conv2D(64, (2, 2), activation=LeakyReLU())\n",
    "        self.conv3 = layers.Conv2D(96, (2, 2), activation=LeakyReLU())\n",
    "        self.conv4 = layers.Conv2D(128, (2, 2), activation=LeakyReLU())\n",
    "        self.conv5 = layers.Conv2D(160, (2, 2), activation=LeakyReLU())\n",
    "        self.conv6 = layers.Conv2D(192, (2, 2), activation=LeakyReLU())\n",
    "\n",
    "        # Six fractional max pooling layers\n",
    "        self.pooling_ratio1 = [1.0, 31/22, 31/22, 1.0]\n",
    "        self.pooling_ratio2 = [1.0, 21/15, 21/15, 1.0]\n",
    "        self.pooling_ratio3 = [1.0, 14/10, 14/10, 1.0]\n",
    "        self.pooling_ratio4 = [1.0, 9/6, 9/6, 1.0]\n",
    "        self.pooling_ratio5 = [1.0, 5/4, 5/4, 1.0]\n",
    "        self.pooling_ratio6 = [1.0, 3/2, 3/2, 1.0]\n",
    "\n",
    "        # Two final convolutional layers\n",
    "        self.conv7 = layers.Conv2D(192, (2, 2), activation=LeakyReLU()) # 4x4\n",
    "        self.conv8 = layers.Conv2D(192, (1, 1), activation=LeakyReLU())\n",
    "\n",
    "        # Linearly increasing dropout in the hidden layers\n",
    "        self.dropout = layers.Dropout(0.5)  # 50% dropout in the final hidden layer\n",
    "\n",
    "        # Two Classifiers \n",
    "        self.fc1 = layers.Dense(5, activation='softmax')  # First half\n",
    "        self.fc2 = layers.Dense(5, activation='softmax')  # Second half\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Feature Extractor\n",
    "        x = self.conv1(inputs) # 31x31\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio1, pseudo_random=True, overlapping=True) # 22x22\n",
    "        x = self.conv2(x) # 21x21\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio2, pseudo_random=True, overlapping=True) # 15x15\n",
    "        x = self.conv3(x) # 14x14\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio3, pseudo_random=True, overlapping=True) # 10x10\n",
    "        x = self.conv4(x) # 9x9\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio4, pseudo_random=True, overlapping=True) # 6x6\n",
    "        x = self.conv5(x) #5x5\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio5, pseudo_random=True, overlapping=True) # 4x4\n",
    "        x = self.conv6(x) # 3x3\n",
    "        x, _, _ = fractional_max_pool(x, self.pooling_ratio6, pseudo_random=True, overlapping=True) # 2x2\n",
    "        x = self.conv7(x) # 1x1\n",
    "        # IDEA: dropout here at 25%\n",
    "        x = self.conv8(x) #1x1\n",
    "\n",
    "        # Classifier\n",
    "        x = layers.Flatten()(x) # future improvement might be to use global average pooling to lower the FCN parameter count\n",
    "        x = self.dropout(x)\n",
    "        output1 = self.fc1(x)  # First half\n",
    "        output2 = self.fc2(x)  # Second half\n",
    "\n",
    "        return {'output1': output1, 'output2': output2}\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Serialize the configuration of the model\n",
    "        config = super(FMP3, self).get_config()\n",
    "        config.update({\n",
    "            'conv1': self.conv1.get_config(),\n",
    "            'conv2': self.conv2.get_config(),\n",
    "            'conv3': self.conv3.get_config(),\n",
    "            'conv4': self.conv4.get_config(),\n",
    "            'conv5': self.conv5.get_config(),\n",
    "            'conv6': self.conv6.get_config(),\n",
    "            'conv7': self.conv7.get_config(),\n",
    "            'conv8': self.conv8.get_config(),\n",
    "            'dropout': self.dropout.get_config(),\n",
    "            'fc1': self.fc1.get_config(),\n",
    "            'fc2': self.fc2.get_config(),\n",
    "            'pooling_ratios': [\n",
    "                self.pooling_ratio1,\n",
    "                self.pooling_ratio2,\n",
    "                self.pooling_ratio3,\n",
    "                self.pooling_ratio4,\n",
    "                self.pooling_ratio5,\n",
    "                self.pooling_ratio6\n",
    "            ]\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'fmp3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"fmp3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"fmp3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ ?                      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,672</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">82,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">123,072</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,648</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">965</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">965</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ ?                      │           \u001b[38;5;34m416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │        \u001b[38;5;34m24,672\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │        \u001b[38;5;34m49,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │        \u001b[38;5;34m82,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │       \u001b[38;5;34m123,072\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │       \u001b[38;5;34m147,648\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │        \u001b[38;5;34m37,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │           \u001b[38;5;34m965\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │           \u001b[38;5;34m965\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">474,410</span> (1.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m474,410\u001b[0m (1.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">474,410</span> (1.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m474,410\u001b[0m (1.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = FMP3()\n",
    "model.build(input_shape=(None, 32, 32, 3))\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'output1': 'categorical_crossentropy', 'output2': 'categorical_crossentropy'},\n",
    "              metrics={'output1': 'accuracy', 'output2': 'accuracy'})\n",
    "model(tf.random.normal((1, 32, 32, 3)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [answer](https://stackoverflow.com/questions/55908188/this-model-has-not-yet-been-built-error-on-model-summary) with 39 upvotes explains why this summary is a bit strange (Keras subclasses Model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 3.2640 - output1_accuracy: 0.1915 - output2_accuracy: 0.1730\n",
      "Epoch 2/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 3.1825 - output1_accuracy: 0.3210 - output2_accuracy: 0.1944\n",
      "Epoch 3/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 3.2185 - output1_accuracy: 0.2629 - output2_accuracy: 0.2418\n",
      "Epoch 4/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 3.2213 - output1_accuracy: 0.3148 - output2_accuracy: 0.2314\n",
      "Epoch 5/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 3.1669 - output1_accuracy: 0.3013 - output2_accuracy: 0.2873\n"
     ]
    }
   ],
   "source": [
    "# Generate random dummy data (e.g., 100 samples, 32x32 RGB images)\n",
    "X_train = np.random.randn(100, 32, 32, 3).astype(np.float32)\n",
    "y_train1 = np.random.randint(0, 5, 100)  # Random labels for output 1\n",
    "y_train2 = np.random.randint(0, 5, 100)  # Random labels for output 2\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train1 = tf.keras.utils.to_categorical(y_train1, 5)\n",
    "y_train2 = tf.keras.utils.to_categorical(y_train2, 5)\n",
    "\n",
    "# Train the model directly using the data (no generator)\n",
    "history = model.fit(X_train, {'output1': y_train1, 'output2':y_train2}, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had a long and painful time figuring out why my generator was not working with fit. Finally I understood that the output of the generator must have a dictionary whose entries match those of the model when compiled!\n",
    "https://stackoverflow.com/questions/44036971/multiple-outputs-in-keras\n",
    "Before that I went down a rabbit hole with older versions of keras which require [fit_generator](https://www.geeksforgeeks.org/keras-fit-and-keras-fit_generator/) because of a stack exchange [post](https://datascience.stackexchange.com/questions/67266/how-to-write-a-generator-for-keras-fit-generator).\n",
    "I read the keras documentation and confirmed that generators are in fact, [well supported](https://keras.io/api/models/model_training_apis/). This hinted to me that the generator would probably work for a simpler single input, single output model.\n",
    "I researched how to create [my own data generator class](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly) that inherits from Keras Sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 98ms/step - loss: 3.0398 - output1_accuracy: 0.2971 - output2_accuracy: 0.3073\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(traingen, epochs=1, steps_per_epoch=len(cifar10_x_train)//batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training with [checkpoints](https://keras.io/api/callbacks/model_checkpoint/)! Its easy! Also add early stopping! I look at my history to verify the names of my scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.8955788612365723],\n",
       " 'output1_accuracy': [0.34895092248916626],\n",
       " 'output2_accuracy': [0.37237730622291565]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implement a custom model callback to save the model weights when the average accuracy improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback which does not use loss as a proxy!\n",
    "class MeanAccModelCheckpoint(Callback):\n",
    "    def __init__(self, filepath, monitor1='output1_accuracy', monitor2='output2_accuracy', mode='max', verbose=1):\n",
    "        super(MeanAccModelCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.monitor1 = monitor1\n",
    "        self.monitor2 = monitor2\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.best_score = -float('inf') if mode == 'max' else float('inf')\n",
    "\n",
    "    # Called at the end of an epoch during training\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        acc1 = logs.get(self.monitor1)\n",
    "        acc2 = logs.get(self.monitor2)\n",
    "\n",
    "        # Compute the average accuracy\n",
    "        avg_accuracy = (acc1 + acc2) / 2 if acc1 is not None and acc2 is not None else None\n",
    "\n",
    "        if avg_accuracy is not None:\n",
    "            if (self.mode == 'max' and avg_accuracy > self.best_score) or \\\n",
    "               (self.mode == 'min' and avg_accuracy < self.best_score):\n",
    "                # Update the best score\n",
    "                self.best_score = avg_accuracy\n",
    "                \n",
    "                # Format the filepath\n",
    "                save_path = self.filepath.format(\n",
    "                    output1_accuracy=acc1,\n",
    "                    output2_accuracy=acc2,\n",
    "                    epoch=epoch + 1,  # Epoch is zero-indexed\n",
    "                    loss=logs.get('loss')\n",
    "                )\n",
    "                \n",
    "                # Save the model\n",
    "                if self.verbose:\n",
    "                    print(f\"\\nEpoch {epoch + 1}: Average accuracy improved to {avg_accuracy:.4f}, saving model to {save_path}\")\n",
    "                self.model.save(save_path)\n",
    "            elif self.verbose:\n",
    "                print(f\"\\nEpoch {epoch + 1}: Average accuracy did not improve (current: {avg_accuracy:.4f}, best: {self.best_score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model_checkpoint_callback = MeanAccModelCheckpoint(\n",
    "    filepath=\"./FMP3/{output1_accuracy:.2f}-{output2_accuracy:.2f}-epoch{epoch:02d}-loss{loss:.2f}.keras\",\n",
    "    monitor1='val_output1_accuracy',\n",
    "    monitor2='val_output2_accuracy',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "val_early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',  # Track the validation loss\n",
    "                               patience=10,         # Number of epochs to wait after the last improvement\n",
    "                               mode='min',         # Stop when the value stops decreasing (minimization)\n",
    "                               restore_best_weights=True,  # Restore the best weights when stopping\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: Average accuracy improved to 0.6707, saving model to ./FMP3/0.63-0.71-epoch01-loss1.68.keras\n",
      "703/703 - 69s - 97ms/step - loss: 1.6830 - output1_accuracy: 0.6299 - output2_accuracy: 0.7209 - val_loss: 1.7101 - val_output1_accuracy: 0.6340 - val_output2_accuracy: 0.7073\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: Average accuracy did not improve (current: 0.6627, best: 0.6707)\n",
      "703/703 - 68s - 97ms/step - loss: 1.6612 - output1_accuracy: 0.6377 - output2_accuracy: 0.7221 - val_loss: 1.7644 - val_output1_accuracy: 0.6196 - val_output2_accuracy: 0.7057\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: Average accuracy improved to 0.6812, saving model to ./FMP3/0.64-0.72-epoch03-loss1.67.keras\n",
      "703/703 - 69s - 98ms/step - loss: 1.6667 - output1_accuracy: 0.6369 - output2_accuracy: 0.7217 - val_loss: 1.6689 - val_output1_accuracy: 0.6390 - val_output2_accuracy: 0.7234\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: Average accuracy did not improve (current: 0.6718, best: 0.6812)\n",
      "703/703 - 69s - 98ms/step - loss: 1.6482 - output1_accuracy: 0.6349 - output2_accuracy: 0.7263 - val_loss: 1.6992 - val_output1_accuracy: 0.6288 - val_output2_accuracy: 0.7147\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: Average accuracy did not improve (current: 0.6708, best: 0.6812)\n",
      "703/703 - 69s - 98ms/step - loss: 1.6448 - output1_accuracy: 0.6378 - output2_accuracy: 0.7281 - val_loss: 1.7147 - val_output1_accuracy: 0.6322 - val_output2_accuracy: 0.7093\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: Average accuracy did not improve (current: 0.6747, best: 0.6812)\n",
      "703/703 - 70s - 99ms/step - loss: 1.6402 - output1_accuracy: 0.6430 - output2_accuracy: 0.7297 - val_loss: 1.7124 - val_output1_accuracy: 0.6332 - val_output2_accuracy: 0.7161\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: Average accuracy did not improve (current: 0.6658, best: 0.6812)\n",
      "703/703 - 70s - 99ms/step - loss: 1.6370 - output1_accuracy: 0.6409 - output2_accuracy: 0.7293 - val_loss: 1.7561 - val_output1_accuracy: 0.6232 - val_output2_accuracy: 0.7083\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: Average accuracy did not improve (current: 0.6678, best: 0.6812)\n",
      "703/703 - 70s - 100ms/step - loss: 1.6382 - output1_accuracy: 0.6456 - output2_accuracy: 0.7277 - val_loss: 1.7547 - val_output1_accuracy: 0.6338 - val_output2_accuracy: 0.7017\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: Average accuracy did not improve (current: 0.6721, best: 0.6812)\n",
      "703/703 - 71s - 101ms/step - loss: 1.6140 - output1_accuracy: 0.6466 - output2_accuracy: 0.7352 - val_loss: 1.7365 - val_output1_accuracy: 0.6206 - val_output2_accuracy: 0.7236\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: Average accuracy improved to 0.6841, saving model to ./FMP3/0.64-0.73-epoch10-loss1.63.keras\n",
      "703/703 - 71s - 101ms/step - loss: 1.6285 - output1_accuracy: 0.6405 - output2_accuracy: 0.7333 - val_loss: 1.6441 - val_output1_accuracy: 0.6364 - val_output2_accuracy: 0.7318\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: Average accuracy did not improve (current: 0.6655, best: 0.6841)\n",
      "703/703 - 72s - 103ms/step - loss: 1.6173 - output1_accuracy: 0.6442 - output2_accuracy: 0.7348 - val_loss: 1.7695 - val_output1_accuracy: 0.6316 - val_output2_accuracy: 0.6993\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: Average accuracy did not improve (current: 0.6740, best: 0.6841)\n",
      "703/703 - 73s - 103ms/step - loss: 1.6025 - output1_accuracy: 0.6465 - output2_accuracy: 0.7356 - val_loss: 1.7141 - val_output1_accuracy: 0.6302 - val_output2_accuracy: 0.7177\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: Average accuracy did not improve (current: 0.6834, best: 0.6841)\n",
      "703/703 - 72s - 102ms/step - loss: 1.6018 - output1_accuracy: 0.6466 - output2_accuracy: 0.7390 - val_loss: 1.6382 - val_output1_accuracy: 0.6386 - val_output2_accuracy: 0.7282\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: Average accuracy did not improve (current: 0.6717, best: 0.6841)\n",
      "703/703 - 72s - 103ms/step - loss: 1.6024 - output1_accuracy: 0.6456 - output2_accuracy: 0.7373 - val_loss: 1.7254 - val_output1_accuracy: 0.6270 - val_output2_accuracy: 0.7163\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: Average accuracy did not improve (current: 0.6816, best: 0.6841)\n",
      "703/703 - 73s - 103ms/step - loss: 1.5975 - output1_accuracy: 0.6505 - output2_accuracy: 0.7369 - val_loss: 1.6747 - val_output1_accuracy: 0.6452 - val_output2_accuracy: 0.7179\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: Average accuracy did not improve (current: 0.6711, best: 0.6841)\n",
      "703/703 - 73s - 104ms/step - loss: 1.5927 - output1_accuracy: 0.6522 - output2_accuracy: 0.7364 - val_loss: 1.7076 - val_output1_accuracy: 0.6320 - val_output2_accuracy: 0.7101\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: Average accuracy did not improve (current: 0.6720, best: 0.6841)\n",
      "703/703 - 74s - 105ms/step - loss: 1.5962 - output1_accuracy: 0.6490 - output2_accuracy: 0.7367 - val_loss: 1.7189 - val_output1_accuracy: 0.6322 - val_output2_accuracy: 0.7117\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: Average accuracy did not improve (current: 0.6731, best: 0.6841)\n",
      "703/703 - 73s - 104ms/step - loss: 1.5859 - output1_accuracy: 0.6531 - output2_accuracy: 0.7384 - val_loss: 1.7184 - val_output1_accuracy: 0.6426 - val_output2_accuracy: 0.7035\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: Average accuracy improved to 0.6849, saving model to ./FMP3/0.64-0.73-epoch19-loss1.57.keras\n",
      "703/703 - 73s - 104ms/step - loss: 1.5670 - output1_accuracy: 0.6555 - output2_accuracy: 0.7427 - val_loss: 1.6410 - val_output1_accuracy: 0.6366 - val_output2_accuracy: 0.7332\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: Average accuracy improved to 0.6889, saving model to ./FMP3/0.66-0.72-epoch20-loss1.57.keras\n",
      "703/703 - 74s - 105ms/step - loss: 1.5738 - output1_accuracy: 0.6555 - output2_accuracy: 0.7431 - val_loss: 1.6384 - val_output1_accuracy: 0.6552 - val_output2_accuracy: 0.7226\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: Average accuracy did not improve (current: 0.6885, best: 0.6889)\n",
      "703/703 - 74s - 105ms/step - loss: 1.5667 - output1_accuracy: 0.6541 - output2_accuracy: 0.7469 - val_loss: 1.6245 - val_output1_accuracy: 0.6478 - val_output2_accuracy: 0.7292\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: Average accuracy did not improve (current: 0.6842, best: 0.6889)\n",
      "703/703 - 74s - 105ms/step - loss: 1.5729 - output1_accuracy: 0.6550 - output2_accuracy: 0.7421 - val_loss: 1.6747 - val_output1_accuracy: 0.6380 - val_output2_accuracy: 0.7304\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: Average accuracy improved to 0.6935, saving model to ./FMP3/0.65-0.73-epoch23-loss1.57.keras\n",
      "703/703 - 76s - 107ms/step - loss: 1.5720 - output1_accuracy: 0.6544 - output2_accuracy: 0.7421 - val_loss: 1.5966 - val_output1_accuracy: 0.6540 - val_output2_accuracy: 0.7330\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: Average accuracy did not improve (current: 0.6909, best: 0.6935)\n",
      "703/703 - 76s - 108ms/step - loss: 1.5644 - output1_accuracy: 0.6564 - output2_accuracy: 0.7452 - val_loss: 1.6360 - val_output1_accuracy: 0.6506 - val_output2_accuracy: 0.7312\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: Average accuracy did not improve (current: 0.6726, best: 0.6935)\n",
      "703/703 - 75s - 106ms/step - loss: 1.5472 - output1_accuracy: 0.6615 - output2_accuracy: 0.7480 - val_loss: 1.7201 - val_output1_accuracy: 0.6162 - val_output2_accuracy: 0.7290\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: Average accuracy did not improve (current: 0.6833, best: 0.6935)\n",
      "703/703 - 75s - 106ms/step - loss: 1.5525 - output1_accuracy: 0.6593 - output2_accuracy: 0.7454 - val_loss: 1.6668 - val_output1_accuracy: 0.6412 - val_output2_accuracy: 0.7254\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: Average accuracy did not improve (current: 0.6833, best: 0.6935)\n",
      "703/703 - 74s - 106ms/step - loss: 1.5514 - output1_accuracy: 0.6615 - output2_accuracy: 0.7463 - val_loss: 1.6660 - val_output1_accuracy: 0.6524 - val_output2_accuracy: 0.7141\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: Average accuracy improved to 0.6938, saving model to ./FMP3/0.65-0.74-epoch28-loss1.55.keras\n",
      "703/703 - 75s - 107ms/step - loss: 1.5487 - output1_accuracy: 0.6593 - output2_accuracy: 0.7483 - val_loss: 1.6177 - val_output1_accuracy: 0.6520 - val_output2_accuracy: 0.7356\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: Average accuracy did not improve (current: 0.6841, best: 0.6938)\n",
      "703/703 - 75s - 107ms/step - loss: 1.5344 - output1_accuracy: 0.6636 - output2_accuracy: 0.7508 - val_loss: 1.6543 - val_output1_accuracy: 0.6354 - val_output2_accuracy: 0.7328\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: Average accuracy did not improve (current: 0.6897, best: 0.6938)\n",
      "703/703 - 74s - 106ms/step - loss: 1.5347 - output1_accuracy: 0.6654 - output2_accuracy: 0.7496 - val_loss: 1.6440 - val_output1_accuracy: 0.6446 - val_output2_accuracy: 0.7348\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: Average accuracy did not improve (current: 0.6897, best: 0.6938)\n",
      "703/703 - 75s - 107ms/step - loss: 1.5428 - output1_accuracy: 0.6616 - output2_accuracy: 0.7493 - val_loss: 1.6089 - val_output1_accuracy: 0.6500 - val_output2_accuracy: 0.7294\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: Average accuracy did not improve (current: 0.6691, best: 0.6938)\n",
      "703/703 - 75s - 107ms/step - loss: 1.5405 - output1_accuracy: 0.6588 - output2_accuracy: 0.7496 - val_loss: 1.7155 - val_output1_accuracy: 0.6342 - val_output2_accuracy: 0.7039\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: Average accuracy improved to 0.6959, saving model to ./FMP3/0.65-0.74-epoch33-loss1.54.keras\n",
      "703/703 - 76s - 108ms/step - loss: 1.5388 - output1_accuracy: 0.6612 - output2_accuracy: 0.7473 - val_loss: 1.5881 - val_output1_accuracy: 0.6502 - val_output2_accuracy: 0.7416\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: Average accuracy did not improve (current: 0.6943, best: 0.6959)\n",
      "703/703 - 76s - 108ms/step - loss: 1.5346 - output1_accuracy: 0.6625 - output2_accuracy: 0.7495 - val_loss: 1.5883 - val_output1_accuracy: 0.6486 - val_output2_accuracy: 0.7400\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: Average accuracy did not improve (current: 0.6872, best: 0.6959)\n",
      "703/703 - 75s - 107ms/step - loss: 1.5196 - output1_accuracy: 0.6686 - output2_accuracy: 0.7530 - val_loss: 1.6475 - val_output1_accuracy: 0.6522 - val_output2_accuracy: 0.7222\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 36: Average accuracy did not improve (current: 0.6885, best: 0.6959)\n",
      "703/703 - 74s - 106ms/step - loss: 1.5341 - output1_accuracy: 0.6649 - output2_accuracy: 0.7504 - val_loss: 1.6060 - val_output1_accuracy: 0.6420 - val_output2_accuracy: 0.7350\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 37: Average accuracy improved to 0.7038, saving model to ./FMP3/0.67-0.74-epoch37-loss1.52.keras\n",
      "703/703 - 75s - 107ms/step - loss: 1.5244 - output1_accuracy: 0.6662 - output2_accuracy: 0.7504 - val_loss: 1.5811 - val_output1_accuracy: 0.6687 - val_output2_accuracy: 0.7390\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 38: Average accuracy did not improve (current: 0.6998, best: 0.7038)\n",
      "703/703 - 77s - 109ms/step - loss: 1.4968 - output1_accuracy: 0.6714 - output2_accuracy: 0.7585 - val_loss: 1.5972 - val_output1_accuracy: 0.6538 - val_output2_accuracy: 0.7458\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 39: Average accuracy did not improve (current: 0.6926, best: 0.7038)\n",
      "703/703 - 76s - 108ms/step - loss: 1.5045 - output1_accuracy: 0.6665 - output2_accuracy: 0.7583 - val_loss: 1.6016 - val_output1_accuracy: 0.6458 - val_output2_accuracy: 0.7394\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 40: Average accuracy did not improve (current: 0.6973, best: 0.7038)\n",
      "703/703 - 76s - 108ms/step - loss: 1.5168 - output1_accuracy: 0.6674 - output2_accuracy: 0.7553 - val_loss: 1.5993 - val_output1_accuracy: 0.6558 - val_output2_accuracy: 0.7388\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 41: Average accuracy did not improve (current: 0.7014, best: 0.7038)\n",
      "703/703 - 75s - 107ms/step - loss: 1.5086 - output1_accuracy: 0.6716 - output2_accuracy: 0.7548 - val_loss: 1.6038 - val_output1_accuracy: 0.6524 - val_output2_accuracy: 0.7504\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 42: Average accuracy did not improve (current: 0.6831, best: 0.7038)\n",
      "703/703 - 75s - 107ms/step - loss: 1.5004 - output1_accuracy: 0.6687 - output2_accuracy: 0.7574 - val_loss: 1.6317 - val_output1_accuracy: 0.6374 - val_output2_accuracy: 0.7288\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 43: Average accuracy did not improve (current: 0.7031, best: 0.7038)\n",
      "703/703 - 76s - 108ms/step - loss: 1.5057 - output1_accuracy: 0.6697 - output2_accuracy: 0.7561 - val_loss: 1.5636 - val_output1_accuracy: 0.6671 - val_output2_accuracy: 0.7392\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 44: Average accuracy did not improve (current: 0.6929, best: 0.7038)\n",
      "703/703 - 75s - 107ms/step - loss: 1.4965 - output1_accuracy: 0.6730 - output2_accuracy: 0.7588 - val_loss: 1.5949 - val_output1_accuracy: 0.6416 - val_output2_accuracy: 0.7442\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 45: Average accuracy did not improve (current: 0.6975, best: 0.7038)\n",
      "703/703 - 76s - 108ms/step - loss: 1.5084 - output1_accuracy: 0.6654 - output2_accuracy: 0.7553 - val_loss: 1.5890 - val_output1_accuracy: 0.6558 - val_output2_accuracy: 0.7392\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 46: Average accuracy did not improve (current: 0.6746, best: 0.7038)\n",
      "703/703 - 76s - 108ms/step - loss: 1.4914 - output1_accuracy: 0.6723 - output2_accuracy: 0.7610 - val_loss: 1.7206 - val_output1_accuracy: 0.6104 - val_output2_accuracy: 0.7388\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 47: Average accuracy did not improve (current: 0.6904, best: 0.7038)\n",
      "703/703 - 75s - 107ms/step - loss: 1.5001 - output1_accuracy: 0.6711 - output2_accuracy: 0.7594 - val_loss: 1.6086 - val_output1_accuracy: 0.6583 - val_output2_accuracy: 0.7226\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 48: Average accuracy did not improve (current: 0.6899, best: 0.7038)\n",
      "703/703 - 76s - 108ms/step - loss: 1.5059 - output1_accuracy: 0.6724 - output2_accuracy: 0.7528 - val_loss: 1.6062 - val_output1_accuracy: 0.6370 - val_output2_accuracy: 0.7428\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 49: Average accuracy did not improve (current: 0.6985, best: 0.7038)\n",
      "703/703 - 76s - 107ms/step - loss: 1.4825 - output1_accuracy: 0.6729 - output2_accuracy: 0.7580 - val_loss: 1.5842 - val_output1_accuracy: 0.6530 - val_output2_accuracy: 0.7440\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 50: Average accuracy did not improve (current: 0.6967, best: 0.7038)\n",
      "703/703 - 76s - 108ms/step - loss: 1.5004 - output1_accuracy: 0.6722 - output2_accuracy: 0.7553 - val_loss: 1.6083 - val_output1_accuracy: 0.6673 - val_output2_accuracy: 0.7262\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 51: Average accuracy did not improve (current: 0.6882, best: 0.7038)\n",
      "703/703 - 78s - 111ms/step - loss: 1.4985 - output1_accuracy: 0.6737 - output2_accuracy: 0.7572 - val_loss: 1.6375 - val_output1_accuracy: 0.6494 - val_output2_accuracy: 0.7270\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 52: Average accuracy did not improve (current: 0.6985, best: 0.7038)\n",
      "703/703 - 78s - 110ms/step - loss: 1.4922 - output1_accuracy: 0.6717 - output2_accuracy: 0.7573 - val_loss: 1.5746 - val_output1_accuracy: 0.6633 - val_output2_accuracy: 0.7338\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 53: Average accuracy did not improve (current: 0.6952, best: 0.7038)\n",
      "703/703 - 81s - 115ms/step - loss: 1.4782 - output1_accuracy: 0.6758 - output2_accuracy: 0.7568 - val_loss: 1.6027 - val_output1_accuracy: 0.6575 - val_output2_accuracy: 0.7330\n",
      "Epoch 53: early stopping\n",
      "Restoring model weights from the end of the best epoch: 43.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    traingen,            # Training data generator\n",
    "    epochs=100,          # Number of epochs to train\n",
    "    steps_per_epoch=len(cifar10_x_train)//batchsize,\n",
    "    validation_data=valgen,  # Validation data generator\n",
    "    validation_steps=len(cifar10_x_val)//batchsize,  # Steps per validation epoch (if using a generator)\n",
    "    callbacks=[val_early_stopping, val_model_checkpoint_callback],  # Early stopping callback\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get a baseline, by evaluating this model according to the project prompt! As always with **tqdm** I first imported it directly, then remembered you have the import the module with the same name as the library. My evaluated model is fresh and initialized with the best trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:31<00:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy =  0.6988849967718125\n",
      "standard deviation =  0.0055302394468447645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(filepath, gen, repeat=1):\n",
    "    model = load_model(filepath)\n",
    "    evaluation_results = []\n",
    "    for i in tqdm(range(repeat)):\n",
    "        loss, acc1, acc2 = model.evaluate(gen, batch_size=10000, steps=1, verbose=False)\n",
    "        evaluation_results.append(np.mean([acc1, acc2]))\n",
    "    print(\"mean accuracy = \", np.mean(evaluation_results))\n",
    "    print(\"standard deviation = \", np.std(evaluation_results))\n",
    "\n",
    "evaluate_model('./FMP3/0.67-0.74-epoch37-loss1.52.keras', testgen, repeat=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:31<00:00,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy =  0.6927750051021576\n",
      "standard deviation =  0.006946627811636093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluation_results = []\n",
    "for i in tqdm(range(10)):\n",
    "    loss, acc1, acc2 = model.evaluate(testgen, batch_size=10000, steps=1, verbose=False)\n",
    "    evaluation_results.append(np.mean([acc1, acc2]))\n",
    "print(\"mean accuracy = \", np.mean(evaluation_results))\n",
    "print(\"standard deviation = \", np.std(evaluation_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.5976195335388184, 0.435699999332428, 0.474700003862381]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.save(filepath='./FMP3.keras')\n",
    "model.evaluate(testgen, batch_size=10000, steps=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel = load_model(filepath='./FMP3.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.5525641441345215, 0.4399000108242035, 0.498199999332428]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testmodel = FMP3()\n",
    "# testmodel.load_weights('./FMP3/model.weights.h5')\n",
    "# testmodel.compile(optimizer='adam',\n",
    "#               loss={'output1': 'categorical_crossentropy', 'output2': 'categorical_crossentropy'},\n",
    "#               metrics={'output1': 'accuracy', 'output2': 'accuracy'})\n",
    "testmodel.evaluate(testgen, batch_size=10000, steps=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, gen, repeat=1):\n",
    "    evaluation_results = []\n",
    "    for i in tqdm(range(repeat)):\n",
    "        loss, acc1, acc2 = model.evaluate(gen, batch_size=10000, steps=1, verbose=False)\n",
    "        evaluation_results.append(np.mean([acc1, acc2]))\n",
    "    print(\"mean accuracy = \", np.mean(evaluation_results))\n",
    "    print(\"standard deviation = \", np.std(evaluation_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy =  0.5836499929428101\n",
      "standard deviation =  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, testgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 1.6299 - output1_accuracy: 0.6460 - output2_accuracy: 0.7316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.629932165145874, 0.6460000276565552, 0.7315999865531921]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testgen, batch_size=10000, steps_per_epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:31<00:00,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy =  0.7097950041294098\n",
      "standard deviation =  0.004811682354824069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "evaluate_model(loaded_model, testgen, repeat=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy =  0.7093549937009811\n",
      "standard deviation =  0.006659487746583883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, testgen, repeat=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For presentation, I need to be able to load the best model from weights so I will preprend this to my evaluate model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not locate class 'FMP3'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'FMP3', 'config': {'trainable': True, 'dtype': 'float32'}, 'registered_name': 'FMP3', 'build_config': {'input_shape': [None, 32, 32, 3]}, 'compile_config': {'optimizer': 'adam', 'loss': {'output1': 'categorical_crossentropy', 'output2': 'categorical_crossentropy'}, 'loss_weights': None, 'metrics': {'output1': ['accuracy'], 'output2': ['accuracy']}, 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean accuracy = \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(evaluation_results))\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandard deviation = \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mstd(evaluation_results))\n\u001b[0;32m---> 12\u001b[0m \u001b[43mevaluate_model_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./ckpt/0.66-0.74-epoch05-loss1.57.model.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[96], line 4\u001b[0m, in \u001b[0;36mevaluate_model_from_path\u001b[0;34m(model_path, gen, repeat)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model_from_path\u001b[39m(model_path, gen, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     evaluation_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(repeat)):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/keras/src/saving/saving_api.py:176\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    173\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    184\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[1;32m    185\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:152\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filename: expected a `.keras` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model_from_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:170\u001b[0m, in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 170\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VARS_FNAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_filenames:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/keras/src/saving/serialization_lib.py:694\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_retrieve_class_or_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregistered_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/keras/src/saving/serialization_lib.py:812\u001b[0m, in \u001b[0;36m_retrieve_class_or_fn\u001b[0;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 812\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure custom classes are decorated with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`@keras.saving.register_keras_serializable()`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not locate class 'FMP3'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'FMP3', 'config': {'trainable': True, 'dtype': 'float32'}, 'registered_name': 'FMP3', 'build_config': {'input_shape': [None, 32, 32, 3]}, 'compile_config': {'optimizer': 'adam', 'loss': {'output1': 'categorical_crossentropy', 'output2': 'categorical_crossentropy'}, 'loss_weights': None, 'metrics': {'output1': ['accuracy'], 'output2': ['accuracy']}, 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False}}"
     ]
    }
   ],
   "source": [
    "\n",
    "# custom_objects={\"FMP3\": FMP3},\n",
    "def evaluate_model_from_path(model_path, gen, repeat=1):\n",
    "    model = load_model(model_path,  compile=True, safe_mode=True)\n",
    "    evaluation_results = []\n",
    "    for i in tqdm(range(repeat)):\n",
    "        loss, acc1, acc2 = model.evaluate(gen, batch_size=10000, steps=1, verbose=False)\n",
    "        evaluation_results.append(np.mean([acc1, acc2]))\n",
    "    print(\"mean accuracy = \", np.mean(evaluation_results))\n",
    "    print(\"standard deviation = \", np.std(evaluation_results))\n",
    "\n",
    "evaluate_model_from_path(\"./ckpt/0.66-0.74-epoch05-loss1.57.model.keras\", testgen, repeat=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to train more, but it bothers me that I am saving models based on loss, so lets write a custom callback which matches our evaluation criteria! I follow this page on [Callbacks](https://keras.io/guides/writing_your_own_callbacks/) and decide I also wish to employ the Tensorboard callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training, I think about data augmentation. I only have 50,000 test samples so if train 100 epochs I for sure am overfitting, need to research which augmentations are successful on CIFAR-10 to help the model generalize. My intuition says mirroring along the vertical axis should be safe. I do not think color augmentations are a good idea, since the colors are already degraded due to the random image merge. As I am thinking of this, I also realize a validation set would help me stop overfitting and allow me to test if data augmentation are helping, so I implement it (at the top of the notebook). Now I need to fit my model with validation accuracies and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: Average accuracy improved to 0.7095, saving model to ./ckpt/0.66-0.76-epoch01-loss1.47.model.keras\n",
      "703/703 - 79s - 112ms/step - loss: 1.4676 - output1_accuracy: 0.6797 - output2_accuracy: 0.7620 - val_loss: 1.5649 - val_output1_accuracy: 0.6639 - val_output2_accuracy: 0.7552\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: Average accuracy did not improve (current: 0.6850, best: 0.7095)\n",
      "703/703 - 78s - 111ms/step - loss: 1.4617 - output1_accuracy: 0.6806 - output2_accuracy: 0.7660 - val_loss: 1.6502 - val_output1_accuracy: 0.6492 - val_output2_accuracy: 0.7208\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: Average accuracy did not improve (current: 0.7048, best: 0.7095)\n",
      "703/703 - 78s - 111ms/step - loss: 1.4598 - output1_accuracy: 0.6822 - output2_accuracy: 0.7641 - val_loss: 1.5201 - val_output1_accuracy: 0.6546 - val_output2_accuracy: 0.7550\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: Average accuracy improved to 0.7184, saving model to ./ckpt/0.67-0.77-epoch04-loss1.47.model.keras\n",
      "703/703 - 78s - 111ms/step - loss: 1.4675 - output1_accuracy: 0.6778 - output2_accuracy: 0.7670 - val_loss: 1.5050 - val_output1_accuracy: 0.6709 - val_output2_accuracy: 0.7660\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: Average accuracy did not improve (current: 0.7021, best: 0.7184)\n",
      "703/703 - 79s - 112ms/step - loss: 1.4617 - output1_accuracy: 0.6803 - output2_accuracy: 0.7664 - val_loss: 1.5552 - val_output1_accuracy: 0.6550 - val_output2_accuracy: 0.7492\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: Average accuracy did not improve (current: 0.7109, best: 0.7184)\n",
      "703/703 - 80s - 113ms/step - loss: 1.4726 - output1_accuracy: 0.6747 - output2_accuracy: 0.7650 - val_loss: 1.5184 - val_output1_accuracy: 0.6793 - val_output2_accuracy: 0.7426\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: Average accuracy did not improve (current: 0.7143, best: 0.7184)\n",
      "703/703 - 79s - 113ms/step - loss: 1.4587 - output1_accuracy: 0.6817 - output2_accuracy: 0.7630 - val_loss: 1.4823 - val_output1_accuracy: 0.6749 - val_output2_accuracy: 0.7538\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: Average accuracy did not improve (current: 0.7053, best: 0.7184)\n",
      "703/703 - 81s - 115ms/step - loss: 1.4535 - output1_accuracy: 0.6826 - output2_accuracy: 0.7655 - val_loss: 1.5529 - val_output1_accuracy: 0.6773 - val_output2_accuracy: 0.7334\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: Average accuracy did not improve (current: 0.7013, best: 0.7184)\n",
      "703/703 - 79s - 112ms/step - loss: 1.4620 - output1_accuracy: 0.6807 - output2_accuracy: 0.7646 - val_loss: 1.5457 - val_output1_accuracy: 0.6579 - val_output2_accuracy: 0.7448\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: Average accuracy did not improve (current: 0.7165, best: 0.7184)\n",
      "703/703 - 78s - 111ms/step - loss: 1.4498 - output1_accuracy: 0.6826 - output2_accuracy: 0.7633 - val_loss: 1.4950 - val_output1_accuracy: 0.6723 - val_output2_accuracy: 0.7608\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: Average accuracy improved to 0.7217, saving model to ./ckpt/0.68-0.77-epoch11-loss1.45.model.keras\n",
      "703/703 - 78s - 111ms/step - loss: 1.4535 - output1_accuracy: 0.6795 - output2_accuracy: 0.7692 - val_loss: 1.4649 - val_output1_accuracy: 0.6783 - val_output2_accuracy: 0.7650\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: Average accuracy did not improve (current: 0.7128, best: 0.7217)\n",
      "703/703 - 78s - 111ms/step - loss: 1.4606 - output1_accuracy: 0.6826 - output2_accuracy: 0.7644 - val_loss: 1.4937 - val_output1_accuracy: 0.6711 - val_output2_accuracy: 0.7546\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: Average accuracy did not improve (current: 0.7104, best: 0.7217)\n",
      "703/703 - 78s - 111ms/step - loss: 1.4448 - output1_accuracy: 0.6834 - output2_accuracy: 0.7679 - val_loss: 1.5230 - val_output1_accuracy: 0.6595 - val_output2_accuracy: 0.7614\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: Average accuracy did not improve (current: 0.7202, best: 0.7217)\n",
      "703/703 - 78s - 111ms/step - loss: 1.4495 - output1_accuracy: 0.6855 - output2_accuracy: 0.7694 - val_loss: 1.4778 - val_output1_accuracy: 0.6813 - val_output2_accuracy: 0.7590\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: Average accuracy did not improve (current: 0.7142, best: 0.7217)\n",
      "703/703 - 79s - 112ms/step - loss: 1.4368 - output1_accuracy: 0.6880 - output2_accuracy: 0.7674 - val_loss: 1.5003 - val_output1_accuracy: 0.6641 - val_output2_accuracy: 0.7644\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: Average accuracy did not improve (current: 0.7205, best: 0.7217)\n",
      "703/703 - 78s - 111ms/step - loss: 1.4304 - output1_accuracy: 0.6859 - output2_accuracy: 0.7733 - val_loss: 1.4936 - val_output1_accuracy: 0.6725 - val_output2_accuracy: 0.7684\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Fit the model with early stopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am very happy with 70.9% accuracy, now I have to make sure saving and loading work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_weights(model, filepath='./test..weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = FMP3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights(filepath='./test..weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
